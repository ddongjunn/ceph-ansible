# ceph-ansible
Ansibleì„ ì‚¬ìš©í•˜ì—¬ **Ceph ìŠ¤í† ë¦¬ì§€ í´ëŸ¬ìŠ¤í„°**ë¥¼ ì‰½ê²Œ ë°°í¬, ì‚­ì œí•  ìˆ˜ ìˆë„ë¡ ìë™í™”ëœ í™˜ê²½ì„ ì œê³µí•©ë‹ˆë‹¤.


## ê°œìš”
- **ğŸ“Œ ëª©ì **: Ceph í´ëŸ¬ìŠ¤í„°ë¥¼ **ìë™í™” ë°©ì‹ìœ¼ë¡œ ë°°í¬ ë° ì‚­ì œ**  
- **ğŸ”§ ì‚¬ìš© ë„êµ¬**: Ansible, Cephadm, Podman/Docker  
- **ğŸ–¥ï¸ ì§€ì› í™˜ê²½**: Ubuntu ê¸°ë°˜ ì‹œìŠ¤í…œ (3ê°œ ì´ìƒì˜ ë…¸ë“œ ì¶”ì²œ)  

## ì°¸ê³ 
- **í…ŒìŠ¤íŠ¸ í™˜ê²½**: Ubuntu 24.04.1 LTS, Ceph v19.2.0  
  - **âš ï¸ ì£¼ì˜**: Ceph ë²„ì „ì— ë”°ë¼ ëª…ë ¹ì–´ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ  
  - **ğŸ› ï¸ í•´ê²° ë°©ë²•**: ì„œë¹„ìŠ¤ ë°°í¬ ë¡œì§ì„ í•„ìš”ì— ë”°ë¼ ìˆ˜ì •  
- **ì£¼ì˜ì‚¬í•­**:  
  - `all_available_devices: true` ì‚¬ìš© ì‹œ ë£¨íŠ¸ ë””ìŠ¤í¬(`/dev/sda`) ì œì™¸ í™•ì¸  
  - `cluster_network` ì„¤ì •ì´ ë…¸ë“œ ê°„ í†µì‹ ì— ë§ëŠ”ì§€ ì ê²€  
- **ì¶”ê°€ ë¬¸ì„œ**:  
  - [Cephadm](https://docs.ceph.com/en/reef/cephadm/)  
  - [Cephadm-ansible](https://github.com/ceph/cephadm-ansible)

## 1ï¸âƒ£ ìš”êµ¬ì‚¬í•­
### í•„ìˆ˜ì„¤ì¹˜
```bash
apt-get update && apt-get install -y ansible sshpass podman
```
### App armor ë¹„í™œì„±í™” (ëª¨ë“  í˜¸ìŠ¤íŠ¸)
```bash
systemctl disable apparmor
service apparmor stop
reboot
```
## 2ï¸âƒ£ í”„ë¡œì íŠ¸ êµ¬ì¡°
```text
CEPH-ANSIBLE/
â”œâ”€â”€ inventory/
â”‚   â”œâ”€â”€ group_vars/
â”‚   â”‚   â”œâ”€â”€ all.yml         # ëª¨ë“  í˜¸ìŠ¤íŠ¸ì— ì ìš©ë˜ëŠ” ì „ì—­ ë³€ìˆ˜
â”‚   â”œâ”€â”€ hosts.ini           # INI í˜•ì‹ì˜ ì¸ë²¤í† ë¦¬ íŒŒì¼
â”‚
â”œâ”€â”€ playbooks/
â”‚   â”œâ”€â”€ deploy.yml          # Ceph í´ëŸ¬ìŠ¤í„° ë°°í¬ í”Œë ˆì´ë¶
â”‚   â”œâ”€â”€ clean.yml           # Ceph í´ëŸ¬ìŠ¤í„° ì‚­ì œ í”Œë ˆì´ë¶
â”‚   â”œâ”€â”€ debug.yml           # ë””ë²„ê¹…ì„ ìœ„í•œ í…ŒìŠ¤íŠ¸ í”Œë ˆì´ë¶
â”‚   â”œâ”€â”€ roles/              # Ansible ì—­í• (Role) ë””ë ‰í† ë¦¬
â”‚   â”‚   â”œâ”€â”€ bootstrap/      # Ceph í´ëŸ¬ìŠ¤í„° ì´ˆê¸° ë¶€íŠ¸ìŠ¤íŠ¸ë© ì—­í• 
â”‚   â”‚   â”‚   â””â”€â”€ tasks/
â”‚   â”‚   â”‚       â””â”€â”€ main.yml
â”‚   â”‚   â”œâ”€â”€ common/         # ê³µí†µ ì‘ì—… ì—­í•  (íŒ¨í‚¤ì§€ ì„¤ì¹˜, ì„¤ì • ë“±)
â”‚   â”‚   â”‚   â””â”€â”€ tasks/
â”‚   â”‚   â”‚       â””â”€â”€ main.yml
â”‚   â”‚   â”œâ”€â”€ health_check/   # Ceph í´ëŸ¬ìŠ¤í„° ìƒíƒœ ì ê²€ ì—­í• 
â”‚   â”‚   â”‚   â””â”€â”€ tasks/
â”‚   â”‚   â”‚       â””â”€â”€ main.yml
â”‚   â”‚   â”œâ”€â”€ services/       # Ceph ì„œë¹„ìŠ¤ ë°°í¬ ì—­í• 
â”‚   â”‚   â”‚   â”œâ”€â”€ tasks/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ main.yml
â”‚   â”‚   â”‚   â”œâ”€â”€ templates/  # ì„œë¹„ìŠ¤ ë°°í¬ë¥¼ ìœ„í•œ Jinja2 í…œí”Œë¦¿
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ iscsi.yaml.j2
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ nvmeof.yaml.j2
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ osd.yaml.j2
â”‚
â”œâ”€â”€ ansible.cfg             # Ansible ì„¤ì • íŒŒì¼
â”œâ”€â”€ cephctl.sh              # Ceph í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ README.md               # í”„ë¡œì íŠ¸ ì„¤ëª… íŒŒì¼
```
## 3ï¸âƒ£ ì„¤ì¹˜ ë° ì„¤ì •
### 1. /etc/hosts íŒŒì¼ ìˆ˜ì • (ê° ë…¸ë“œì—ì„œ ë™ì¼í•˜ê²Œ ì„¤ì •)
```bash
192.168.0.191 squid4
192.168.0.192 squid5
192.168.0.193 squid6
```

### 2. AppArmor ë¹„í™œì„±í™” (ë…¸ë“œ ì „ë¶€)
```bash
systemctl disable apparmor
service apparmor stop
reboot
```

### 3. ë°°í¬ ì „ ì„¤ì •
####  `inventory/hosts.ini` : Ceph í´ëŸ¬ìŠ¤í„°ì— í¬í•¨ë  ë…¸ë“œ ë° ë¶€íŠ¸ìŠ¤íŠ¸ë© ë…¸ë“œë¥¼ ì„¤ì •
```ini
[all]
squid4 ansible_host=192.168.0.191
squid5 ansible_host=192.168.0.192
squid6 ansible_host=192.168.0.193

[bootstrap]
squid4
```
#### `group_vars/all.yml` : ë°°í¬í•  Ceph ì„œë¹„ìŠ¤ ë° ì„¤ì •ì„ ì •ì˜í•©ë‹ˆë‹¤.
```yaml
---
ansible_user: root
ansible_ssh_pass: squid

# í´ëŸ¬ìŠ¤í„° ë°°í¬ ê´€ë ¨ ì„¤ì •
ceph:
  mon_ip: 192.168.0.191
  cluster_network: "10.0.4.0/24"
  version: "quay.io/ceph/ceph:v19.2.0"
  cephadm_version: "19.2.0"
  fsid: "fb2a0676-f439-11ef-82d7-080027b7bc18" # FSID ë¯¸ì§€ì • ì‹œ ìë™ ìƒì„±  
  dashboard:
    init_password: "squid!@#$"

clean: # í´ëŸ¬ìŠ¤í„° ì‚­ì œì‹œ ì‚¬ìš©
  fsid: "fb2a0676-f439-11ef-82d7-080027b7bc18"
  dirs:  # ì‚­ì œí•  ë””ë ‰í† ë¦¬ ëª©ë¡
    - /etc/ceph
    - /var/lib/ceph
    - /var/log/ceph
  packages:  # ì‚­ì œí•  íŒ¨í‚¤ì§€ ëª©ë¡
    - ceph
    - ceph-mgr
    - ceph-mon
    - ceph-osd
    - ceph-mds    
    - cephadm
    - ceph-common

# ë°°í¬í•  ì„œë¹„ìŠ¤ ì •ì˜
services:
  # í•„ìˆ˜ ì„œë¹„ìŠ¤: Ceph í´ëŸ¬ìŠ¤í„°ì˜ ê¸°ë³¸ ë™ì‘ì„ ìœ„í•´ ë°˜ë“œì‹œ ë°°í¬í•´ì•¼ í•¨

  mon:
    placement: "3"  # Monitor: í´ëŸ¬ìŠ¤í„° ìƒíƒœ ê´€ë¦¬

  mgr:
    placement: "2"  # Manager: ê´€ë¦¬ ê¸°ëŠ¥ ë° ëŒ€ì‹œë³´ë“œ ì œê³µ

  osd:
    #ë””ë°”ì´ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ osdë¡œ í™œì„±í™” í•  ê²½ìš° true, íŠ¹ì • ë””ìŠ¤í¬ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ falseë¡œ ì„¤ì •í•˜ê³  devices í•­ëª©ì„ ì‘ì„± (í•„ìˆ˜)
    all_available_devices: true 
    name: "osd.default"  # Object Storage Daemon: ë°ì´í„° ì €ì¥, í•„ìˆ˜
    hosts:
      - "squid4"
      - "squid5"
      - "squid6"
    devices:
      - "/dev/sdb"
      - "/dev/sdc"
      - "/dev/nvme0n1"
      - "/dev/nvme0n2"

  # ì„ íƒ ì„œë¹„ìŠ¤: í•„ìš”ì— ë”°ë¼ ë°°í¬, ì‚¬ìš©í•˜ì§€ ì•Šì„ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸([])ë¡œ ì„¤ì • ê°€ëŠ¥  
  # Metadata Server: CephFS(íŒŒì¼ ìŠ¤í† ë¦¬ì§€) ì‚¬ìš© ì‹œ í•„ìš”
  mds:
    - name: "mds.default"
      placement: "3"
      pool:
        metadata: "cephfs_metadata"
        data: "cephfs_data"

  # RGW (RADOS Gateway) - ê°ì²´ ìŠ¤í† ë¦¬ì§€(S3/Swift)

  rgw:
    - name: "rgw.default"
      placement: "3"
      pool: "rgw_data"
      realm: 
        name: "realm.default"   # RGWê°€ ì†í•  Realm ì´ë¦„
        default: true           # ê¸°ë³¸ Realmìœ¼ë¡œ ì„¤ì •
      zonegroup: 
        name: "zonegroup.default"
        default: true           # ê¸°ë³¸ Zonegroup ì„¤ì •
        master: true            # í•´ë‹¹ Zonegroupì˜ Master ì„¤ì • (ìµœì†Œ 1ê°œ í•„ìš”)
      zone: 
        name: "zone.default"
        master: true            # í•´ë‹¹ Zoneì˜ Master ì„¤ì • (ìµœì†Œ 1ê°œ í•„ìš”)
        default: true           # ê¸°ë³¸ Zoneìœ¼ë¡œ ì„¤ì •
      port: 7480                # RGW ì„œë¹„ìŠ¤ê°€ ì‚¬ìš©í•  í¬íŠ¸ (ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ì„¤ì •)
    - name: "rgw.default2"
      placement: "3"
      pool: "rgw_data2"
      realm:
        name: "realm.default2"
      zonegroup:
        name: "zonegroup.default2"
      zone: 
        name: "zone.default2"
      port: 7481  # ë‘ ë²ˆì§¸ RGW ì¸ìŠ¤í„´ìŠ¤, í¬íŠ¸ ë³€ê²½ í•„ìš” (7481)

  # NFS: CephFS ê¸°ë°˜ NFS ì„œë²„, íŒŒì¼ ìŠ¤í† ë¦¬ì§€ ì‚¬ìš© ì‹œ í•„ìš” (MDS ì˜ì¡´)
  nfs:
    - name: "nfs.default"
      placement: "3"
      pool: "cephfs_data"

  # RBD Mirror: ë¸”ë¡ ìŠ¤í† ë¦¬ì§€ ë¯¸ëŸ¬ë§ ì‚¬ìš© ì‹œ í•„ìš”
  rbd_mirror:
    - placement: "3"
      pool: "rbd_pool"

  # iSCSI Gateway: ë¸”ë¡ ìŠ¤í† ë¦¬ì§€ë¥¼ iSCSIë¡œ ì œê³µ ì‹œ í•„ìš”
  iscsi:
    - name: "iscsi.default"
      pool: "iscsi_pool"
      api_user: "iscsi_admin_user"
      api_password: "ceph!@#$"
      placement:
        hosts:
          - "squid4"
          - "squid5
          - "squid6"

  # NVMe over Fabrics: ê³ ì„±ëŠ¥ ë¸”ë¡ ìŠ¤í† ë¦¬ì§€ ì‚¬ìš© ì‹œ í•„ìš”
  nvmeof: []

  # ëª¨ë‹ˆí„°ë§ ë„êµ¬: í´ëŸ¬ìŠ¤í„° ìƒíƒœ ëª¨ë‹ˆí„°ë§ ì‹œ í•„ìš”
  monitoring:
    prometheus:
      placement: "1"
    grafana:
      api_url: "https://192.168.0.191:3000"
      placement: "1"
    alertmanager:
      placement: "1"
    node_exporter:
      placement: "*"
    crash:
      placement: "*"
```

### ğŸ“Œ ì„œë¹„ìŠ¤ ìœ í˜• ì •ì˜ (`group_vars/all.yml`)  
Cephì˜ ì„œë¹„ìŠ¤ëŠ” **í•„ìˆ˜ ì„œë¹„ìŠ¤**ì™€ **ì„ íƒ ì„œë¹„ìŠ¤**ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.  
ì•„ë˜ ì •ì˜ëœ ê°’ì€ `group_vars/all.yml`ì—ì„œ ì„¤ì •  

---

### **ğŸ”¹ í•„ìˆ˜ ì„œë¹„ìŠ¤ (í´ëŸ¬ìŠ¤í„° ìš´ì˜ì— ë°˜ë“œì‹œ í•„ìš”)**
| ì„œë¹„ìŠ¤  | ì„¤ëª… |
|---------|------|
| **MON (Monitor)** | í´ëŸ¬ìŠ¤í„° ìƒíƒœ ê´€ë¦¬ (ìµœì†Œ 3ê°œ ê¶Œì¥) |
| **MGR (Manager)** | ê´€ë¦¬ ê¸°ëŠ¥ ë° Ceph ëŒ€ì‹œë³´ë“œ ì œê³µ (ìµœì†Œ 2ê°œ ê¶Œì¥) |
| **OSD (Object Storage Daemon)** | ë°ì´í„° ì €ì¥ (í•„ìˆ˜) |

#### **OSD ë°°í¬ ì˜µì…˜**
- `all_available_devices: true` â†’ **ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ OSDë¡œ ë°°í¬** (ë””ìŠ¤í¬ ëª©ë¡ ì„¤ì • ë¶ˆí•„ìš”)
- `all_available_devices: false` â†’ **ì§€ì •í•œ devices ëª©ë¡ë§Œ OSDë¡œ ì‚¬ìš©** (ëª…í™•í•œ ë””ìŠ¤í¬ ì§€ì • í•„ìš”)  

```yaml
osd:
  all_available_devices: true  # trueì´ë©´ ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ ë””ìŠ¤í¬ë¥¼ OSDë¡œ ì„¤ì •
  hosts:
    - "squid4"
    - "squid5"
    - "squid6"
  devices:  # all_available_devicesê°€ falseì¼ ë•Œë§Œ ì‚¬ìš©
    - "/dev/sdb"
    - "/dev/sdc"
    - "/dev/nvme0n1"
    - "/dev/nvme0n2" 
```

### **ğŸ”¹ ì„ íƒ ì„œë¹„ìŠ¤ (ì‚¬ìš©í•˜ì§€ ì•Šì„ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ [] ì„¤ì • ê°€ëŠ¥)**
2ê°œ ì´ìƒ ë™ì¼í•œ ì„œë¹„ìŠ¤ ë°°í¬ì‹œ ë³„ë„ì˜ í¬íŠ¸ ì§€ì •
| ì„œë¹„ìŠ¤ | ì„¤ëª… |
|--------|--------------------------------------------------|
| **ğŸ“‚ MDS (CephFS)** | CephFS(íŒŒì¼ ìŠ¤í† ë¦¬ì§€) ì‚¬ìš© ì‹œ í•„ìš” |
| **ğŸŒ RGW (RADOS Gateway)** | S3 ë° Swift APIë¥¼ ì œê³µí•˜ëŠ” ê°ì²´ ìŠ¤í† ë¦¬ì§€ ì„œë¹„ìŠ¤ |
| **ğŸ“¡ NFS (Network File System)** | CephFS ê¸°ë°˜ NFS ì„œë²„ (MDS ì˜ì¡´) |
| **ğŸ”„ RBD Mirror** | ë¸”ë¡ ìŠ¤í† ë¦¬ì§€ ë¯¸ëŸ¬ë§ (ë©€í‹° í´ëŸ¬ìŠ¤í„° í™˜ê²½ì—ì„œ í•„ìš”) |
| **ğŸ”— iSCSI Gateway** | Ceph ë¸”ë¡ ìŠ¤í† ë¦¬ì§€ë¥¼ iSCSIë¡œ ì œê³µ ì‹œ í•„ìš” |
| **ğŸš€ NVMe-oF (NVMe over Fabrics)** | ê³ ì„±ëŠ¥ ë¸”ë¡ ìŠ¤í† ë¦¬ì§€ |
| **ğŸ“Š Monitoring** | Prometheus, Grafana ê¸°ë°˜ ëª¨ë‹ˆí„°ë§ |

## 4ï¸âƒ£ ì‚¬ìš© ë°©ë²•
### ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
```bash
chmod +x cephctl.sh
```
### Ceph í´ëŸ¬ìŠ¤í„° ë°°í¬
`cephctl.sh` ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ Ceph í´ëŸ¬ìŠ¤í„°ë¥¼ ë°°í¬ ë° ì‚­ì œ
```bash
./cephctl.sh deploy
```
### Ceph í´ëŸ¬ìŠ¤í„° ì‚­ì œ
```bash
./cephctl.sh cleanup

TASK [í´ëŸ¬ìŠ¤í„° ì‚­ì œ í™•ì¸ ìš”ì²­] ****************************************************************
[í´ëŸ¬ìŠ¤í„° ì‚­ì œ í™•ì¸ ìš”ì²­]
[ê²½ê³ ] í´ëŸ¬ìŠ¤í„° ì‚­ì œë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
ì‚­ì œí•  FSID: fb2a0676-f439-11ef-82d7-080027b7bc18
âš ï¸ ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ê³„ì† ì§„í–‰í•˜ë ¤ë©´ "yes"ë¥¼ ì…ë ¥í•˜ì„¸ìš”.
:
yes
```


## 5ï¸âƒ£ íŠ¸ëŸ¬ë¸” ìŠˆíŒ…
### ğŸ“Œ osd ë°°í¬ê°€ ì•ˆë˜ëŠ”ê²½ìš° 
```bash
TASK [services : osd ë°°í¬ ì‹¤íŒ¨] *************************************
fatal: [squid4]: FAILED! => {"changed": false, "msg": "âš ï¸ osd ë°°í¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ! ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."}
```

1. ë””ìŠ¤í¬ ìƒíƒœ í™•ì¸

- `"REJECT REASONS"`ì— `"Has a filesystem"`ì´ í‘œì‹œë˜ë©´, ê¸°ì¡´ íŒŒì¼ ì‹œìŠ¤í…œì´ ì¡´ì¬í•˜ì—¬ Cephì—ì„œ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ìƒíƒœ
- `"AVAILABLE"`ì´ `"No"`ë¡œ ë˜ì–´ ìˆìœ¼ë©´ í•´ë‹¹ ë””ë°”ì´ìŠ¤ë¥¼ OSDë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ
```bash
root@squid4:~/ceph-ansible# cephadm shell -- ceph orch device ls
Inferring fsid fb2a0676-f439-11ef-82d7-080027b7bc18
Inferring config /var/lib/ceph/fb2a0676-f439-11ef-82d7-080027b7bc18/mon.squid4/config
Using ceph image with id '37996728e013' and tag 'v19.2.0' created on 2024-09-27 22:08:21 +0000 UTC
quay.io/ceph/ceph@sha256:200087c35811bf28e8a8073b15fa86c07cce85c575f1ccd62d1d6ddbfdc6770a
HOST    PATH          TYPE  DEVICE ID                               SIZE  AVAILABLE  REFRESHED  REJECT REASONS                                                         
squid4  /dev/nvme0n1  ssd   ORCL-VBOX-NVME-VER12_VB1234-56789      25.0G  Yes        6m ago                                                                            
squid4  /dev/nvme0n2  ssd   ORCL-VBOX-NVME-VER12_VB1234-56789      25.0G  Yes        6m ago                                                                            
squid4  /dev/sdb      hdd   ATA_VBOX_HARDDISK_VBca670075-a90f7e92  25.0G  Yes        6m ago                                                                            
squid4  /dev/sdc      hdd   ATA_VBOX_HARDDISK_VB53734388-019ba057  25.0G  Yes        6m ago                                                                            
squid4  /dev/sr0      hdd   VBOX_CD-ROM_VB0-01f003f6               1023M  No         6m ago     Failed to determine if device is BlueStore, Insufficient space (<5GB)  
squid5  /dev/nvme0n1  ssd   ORCL-VBOX-NVME-VER12_VB1234-56789      25.0G  Yes        6m ago                                                                            
squid5  /dev/nvme0n2  ssd   ORCL-VBOX-NVME-VER12_VB1234-56789      25.0G  Yes        6m ago                                                                            
squid5  /dev/sdb      hdd   ATA_VBOX_HARDDISK_VB9885d691-0cc5bf01  25.0G  Yes        6m ago                                                                            
squid5  /dev/sdc      hdd   ATA_VBOX_HARDDISK_VBf0b509d1-2d420635  25.0G  Yes        6m ago                                                                            
squid5  /dev/sr0      hdd   VBOX_CD-ROM_VB2-01700376               1023M  No         6m ago     Failed to determine if device is BlueStore, Insufficient space (<5GB)  
squid6  /dev/nvme0n1  ssd   ORCL-VBOX-NVME-VER12_VB1234-56789      25.0G  Yes        6m ago                                                                            
squid6  /dev/nvme0n2  ssd   ORCL-VBOX-NVME-VER12_VB1234-56789      25.0G  Yes        6m ago                                                                            
squid6  /dev/sdb      hdd   ATA_VBOX_HARDDISK_VB3046362a-5fedd26f  25.0G  Yes        6m ago                                                                            
squid6  /dev/sdc      hdd   ATA_VBOX_HARDDISK_VB315e39f4-ff71492b  25.0G  Yes        6m ago                                                                            
squid6  /dev/sr0      hdd   VBOX_CD-ROM_VB2-01700376               1023M  No         6m ago     Failed to determine if device is BlueStore, Insufficient space (<5GB)  

```
2. í´ëŸ¬ìŠ¤í„° ì‚­ì œ
```bash
./cephctl.sh cleanup
```

2. AppArmor ì¬í™•ì¸
```bash
cat /sys/module/apparmor/parameters/enabled # Yì¸ ê²½ìš°

mkdir /etc/apparmor.d/disabled/
mv /etc/apparmor.d/MongoDB_Compass /etc/apparmor.d/disabled/
systemctl disable apparmor && service apparmor stop && reboot
```

3. ì¬ë°°í¬
```bash
./cephctl.sh deploy
```

### ğŸ“Œ íŠ¹ì • ë…¸ë“œì—ì„œ ì‚­ì œê°€ ì•ˆë˜ëŠ” ê²½ìš° (í•´ë‹¹ ë…¸ë“œ ì¬ë¶€íŒ… í›„ ìˆ˜ë™ìœ¼ë¡œ ì‚­ì œ)
1. ë…¸ë“œ ì¬ë¶€íŒ…
```bash
ssh root@<ë…¸ë“œ> "reboot"
```
2. Cephadm ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰ ê¶Œí•œ ì„¤ì •
```bash
curl -o /usr/sbin/cephadm https://download.ceph.com/rpm-{{ ceph.cephadm_version }}/el9/noarch/cephadm
chmod 755 /usr/sbin/cephadm
```
3. fsid dir í™•ì¸

í•´ë‹¹ ë…¸ë“œ fsid dir í™•ì¸(Ceph í´ëŸ¬ìŠ¤í„°ì˜ ê³ ìœ  ì‹ë³„ì)ë¥¼ í™•ì¸
```bash
root@squid5:~# ls -al /var/lib/ceph
total 12
drwxr-xr-x  3 root root 4096 Feb 27 10:58 .
drwxr-xr-x 47 root root 4096 Feb 27 04:36 ..
drwx------  8  167  167 4096 Feb 27 10:59 fb2a0676-f439-11ef-82d7-080027b7bc18
```

4. í´ëŸ¬ìŠ¤í„° ê°•ì œ ì‚­ì œ
```bash
cephadm rm-cluster --force --zap-osds --fsid "{{ clean.fsid }}"
```
---
ì‚¬ìš©í•˜ì‹œë©´ì„œ ì´ìŠˆë‚˜ ê°œì„  ì‚¬í•­ì´ ìˆìœ¼ë©´ PR ë˜ëŠ” Amaranthë¡œ ê³µìœ  ë¶€íƒë“œë¦½ë‹ˆë‹¤! ğŸ™Œ
